"""
Talking Head / Mouth Sync (Lightweight - No ML Models)
Animate mouth based on audio energy analysis
"""

import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from dataclasses import dataclass
from loguru import logger

try:
    import librosa
    LIBROSA_AVAILABLE = True
except ImportError:
    librosa = None
    LIBROSA_AVAILABLE = False
    logger.warning("librosa not installed, using fallback audio analysis")


@dataclass
class MouthFrame:
    """Single frame mouth state"""
    frame: int
    mouth: str  # "open" or "closed"
    intensity: float  # 0.0 to 1.0

    def to_dict(self) -> Dict:
        return {
            "frame": self.frame,
            "mouth": self.mouth,
            "intensity": self.intensity
        }


class MouthSyncGenerator:
    """Generate mouth animation from audio without ML models"""

    def __init__(
        self,
        silence_threshold: float = 0.02,
        smoothing_window: int = 3,
        min_open_duration: int = 2
    ):
        self.silence_threshold = silence_threshold
        self.smoothing_window = smoothing_window
        self.min_open_duration = min_open_duration

    def generate_mouth_animation(
        self,
        audio_path: str,
        fps: int = 24
    ) -> Tuple[List[Dict], float]:
        """
        Analyze audio and generate per-frame mouth states

        Returns:
            Tuple of (frame_animation_map, confidence_score)
        """
        if not Path(audio_path).exists():
            logger.error(f"Audio file not found: {audio_path}")
            return [], 0.0

        # Load and analyze audio
        if LIBROSA_AVAILABLE:
            rms_per_frame, duration = self._analyze_with_librosa(audio_path, fps)
        else:
            rms_per_frame, duration = self._analyze_fallback(audio_path, fps)

        if len(rms_per_frame) == 0:
            return [], 0.0

        # Normalize RMS values
        rms_normalized = self._normalize_rms(rms_per_frame)

        # Apply smoothing
        rms_smoothed = self._smooth_signal(rms_normalized)

        # Generate mouth states
        mouth_states = self._determine_mouth_states(rms_smoothed)

        # Apply minimum duration filtering
        mouth_states = self._apply_duration_filter(mouth_states)

        # Create frame animation map
        animation_map = []
        for frame_idx, (state, intensity) in enumerate(mouth_states):
            animation_map.append(MouthFrame(
                frame=frame_idx,
                mouth=state,
                intensity=intensity
            ).to_dict())

        # Calculate confidence score
        confidence = self._calculate_confidence(rms_smoothed, mouth_states)

        logger.info(f"âœ… Generated {len(animation_map)} mouth frames, confidence: {confidence:.2f}")

        return animation_map, confidence

    def _analyze_with_librosa(
        self,
        audio_path: str,
        fps: int
    ) -> Tuple[np.ndarray, float]:
        """Analyze audio using librosa"""
        try:
            # Load audio
            y, sr = librosa.load(audio_path, sr=None)
            duration = len(y) / sr

            # Calculate frame-aligned RMS
            hop_length = int(sr / fps)
            rms = librosa.feature.rms(y=y, hop_length=hop_length)[0]

            return rms, duration
        except Exception as e:
            logger.error(f"Librosa analysis failed: {e}")
            return np.array([]), 0.0

    def _analyze_fallback(
        self,
        audio_path: str,
        fps: int
    ) -> Tuple[np.ndarray, float]:
        """Fallback audio analysis without librosa"""
        try:
            import wave
            import struct

            # Try to convert MP3 to WAV first if needed
            if audio_path.lower().endswith('.mp3'):
                return self._analyze_mp3_fallback(audio_path, fps)

            with wave.open(audio_path, 'rb') as wav:
                n_channels = wav.getnchannels()
                sample_width = wav.getsampwidth()
                frame_rate = wav.getframerate()
                n_frames = wav.getnframes()

                duration = n_frames / frame_rate
                samples_per_video_frame = int(frame_rate / fps)

                raw_data = wav.readframes(n_frames)

                # Convert to samples
                if sample_width == 2:
                    fmt = f"<{n_frames * n_channels}h"
                    samples = np.array(struct.unpack(fmt, raw_data))
                else:
                    samples = np.frombuffer(raw_data, dtype=np.int16)

                # Convert to mono if stereo
                if n_channels == 2:
                    samples = samples.reshape(-1, 2).mean(axis=1)

                # Calculate RMS per video frame
                rms_values = []
                for i in range(0, len(samples), samples_per_video_frame):
                    chunk = samples[i:i + samples_per_video_frame]
                    if len(chunk) > 0:
                        rms = np.sqrt(np.mean(chunk.astype(float) ** 2))
                        rms_values.append(rms)

                return np.array(rms_values), duration

        except Exception as e:
            logger.error(f"Fallback audio analysis failed: {e}")
            return np.array([]), 0.0

    def _analyze_mp3_fallback(
        self,
        audio_path: str,
        fps: int
    ) -> Tuple[np.ndarray, float]:
        """Analyze MP3 file using pydub or simple estimation"""
        try:
            from pydub import AudioSegment

            audio = AudioSegment.from_mp3(audio_path)
            duration = len(audio) / 1000.0  # ms to seconds

            # Get samples
            samples = np.array(audio.get_array_of_samples())
            if audio.channels == 2:
                samples = samples.reshape(-1, 2).mean(axis=1)

            sample_rate = audio.frame_rate
            samples_per_frame = int(sample_rate / fps)

            rms_values = []
            for i in range(0, len(samples), samples_per_frame):
                chunk = samples[i:i + samples_per_frame]
                if len(chunk) > 0:
                    rms = np.sqrt(np.mean(chunk.astype(float) ** 2))
                    rms_values.append(rms)

            return np.array(rms_values), duration

        except ImportError:
            logger.warning("pydub not available, using simple duration estimation")
            # Simple estimation based on file size
            import os
            file_size = os.path.getsize(audio_path)
            # Rough estimate: ~16KB per second for 128kbps MP3
            duration = file_size / 16000
            num_frames = int(duration * fps)

            # Create random-ish RMS values (won't be accurate but won't crash)
            rms_values = np.random.uniform(0.1, 0.5, num_frames)
            return rms_values, duration
        except Exception as e:
            logger.error(f"MP3 analysis failed: {e}")
            return np.array([]), 0.0

    def _normalize_rms(self, rms: np.ndarray) -> np.ndarray:
        """Normalize RMS values to 0-1 range"""
        if len(rms) == 0:
            return rms

        min_val = rms.min()
        max_val = rms.max()

        if max_val - min_val < 1e-6:
            return np.zeros_like(rms)

        return (rms - min_val) / (max_val - min_val)

    def _smooth_signal(self, signal: np.ndarray) -> np.ndarray:
        """Apply moving average smoothing"""
        if len(signal) < self.smoothing_window:
            return signal

        kernel = np.ones(self.smoothing_window) / self.smoothing_window
        smoothed = np.convolve(signal, kernel, mode='same')

        return smoothed

    def _determine_mouth_states(
        self,
        rms: np.ndarray
    ) -> List[Tuple[str, float]]:
        """Determine mouth open/closed states"""
        states = []

        for intensity in rms:
            if intensity > self.silence_threshold:
                states.append(("open", float(intensity)))
            else:
                states.append(("closed", float(intensity)))

        return states

    def _apply_duration_filter(
        self,
        states: List[Tuple[str, float]]
    ) -> List[Tuple[str, float]]:
        """Filter out very short state changes"""
        if len(states) < self.min_open_duration:
            return states

        filtered = list(states)

        i = 0
        while i < len(filtered):
            current_state = filtered[i][0]

            # Find run length
            run_length = 1
            while (i + run_length < len(filtered) and
                   filtered[i + run_length][0] == current_state):
                run_length += 1

            # If run is too short and not at edges, flip to surrounding state
            if run_length < self.min_open_duration:
                if i > 0 and i + run_length < len(filtered):
                    prev_state = filtered[i - 1][0]
                    for j in range(run_length):
                        filtered[i + j] = (prev_state, filtered[i + j][1])

            i += run_length

        return filtered

    def _calculate_confidence(
        self,
        rms: np.ndarray,
        states: List[Tuple[str, float]]
    ) -> float:
        """Calculate confidence score for the animation"""
        if len(rms) == 0:
            return 0.0

        # Factors affecting confidence:
        # 1. Signal-to-noise ratio
        speech_frames = sum(1 for s, _ in states if s == "open")
        speech_ratio = speech_frames / len(states) if len(states) > 0 else 0

        # 2. Dynamic range
        dynamic_range = rms.max() - rms.min() if len(rms) > 0 else 0

        # 3. Variance in the signal
        variance = np.var(rms) if len(rms) > 0 else 0

        # Combine factors
        confidence = min(1.0, (
            0.3 * min(1.0, speech_ratio * 2) +
            0.4 * min(1.0, dynamic_range * 2) +
            0.3 * min(1.0, variance * 10)
        ))

        return confidence


def generate_mouth_animation(audio_path: str, fps: int = 24) -> Tuple[List[Dict], float]:
    """Convenience function for mouth animation generation"""
    generator = MouthSyncGenerator()
    return generator.generate_mouth_animation(audio_path, fps)
